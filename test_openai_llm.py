"""Quick test for OpenAI LLM client."""

import asyncio
import os
from dotenv import load_dotenv
from src.openai_llm_client import OpenAILLMClient

async def test_openai_streaming():
    """Test OpenAI streaming with Vietnamese."""
    load_dotenv()
    
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ OPENAI_API_KEY not found in .env")
        return
    
    print("ğŸš€ Testing OpenAI LLM streaming...")
    
    # Create client
    client = OpenAILLMClient(
        api_key=api_key,
        model_name="gpt-4o-mini",
        system_instruction="Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ¢n thiá»‡n, nÃ³i tiáº¿ng Viá»‡t tá»± nhiÃªn vÃ  vui váº».",
        temperature=0.7
    )
    
    # Test message
    message = "Xin chÃ o! Báº¡n lÃ  ai?"
    print(f"\nğŸ‘¤ User: {message}\nğŸ¤– Assistant: ", end="", flush=True)
    
    # Stream response
    full_response = ""
    for chunk in client.chat_stream(message, []):
        print(chunk, end="", flush=True)
        full_response += chunk
    
    print("\n\nâœ… Streaming completed!")
    print(f"ğŸ“Š Total length: {len(full_response)} characters")
    
    # Test with history
    print("\n" + "="*50)
    history = [
        {"role": "user", "content": message},
        {"role": "assistant", "content": full_response}
    ]
    
    message2 = "Ká»ƒ cho tÃ´i má»™t cÃ¢u chuyá»‡n ngáº¯n vui"
    print(f"\nğŸ‘¤ User: {message2}\nğŸ¤– Assistant: ", end="", flush=True)
    
    full_response2 = ""
    for chunk in client.chat_stream(message2, history):
        print(chunk, end="", flush=True)
        full_response2 += chunk
    
    print("\n\nâœ… Second message completed!")
    print(f"ğŸ“Š Total length: {len(full_response2)} characters")

if __name__ == "__main__":
    asyncio.run(test_openai_streaming())
